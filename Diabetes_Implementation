{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeipSk8ZHetn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def load_data():\n",
    "    # Load the diabetes prediction dataset\n",
    "    data = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Convert categorical variables to numerical\n",
    "    data['gender'] = data['gender'].map({'Female': 0, 'Male': 1})\n",
    "    data['smoking_history'] = data['smoking_history'].map({'never': 0, 'No Info': 1, 'current': 2, 'former': 3, 'ever': 4, 'not current': 5})\n",
    "\n",
    "    return data\n",
    "\n",
    "def data_split(data):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def data_scaling(train_x, test_x):\n",
    "    ss = StandardScaler()\n",
    "    ss_model = ss.fit(train_x)\n",
    "    train_x_scaled = ss_model.transform(train_x)\n",
    "    test_x_scaled = ss_model.transform(test_x)\n",
    "    return train_x_scaled, test_x_scaled\n",
    "\n",
    "def data_scaling_plot(data):\n",
    "    X = data.iloc[:, :-1].to_numpy()\n",
    "    y = data.iloc[:, -1].to_numpy()\n",
    "    ss = StandardScaler()\n",
    "    ss_model = ss.fit(X)\n",
    "    X_scaled = ss_model.transform(X)\n",
    "    np_new = np.hstack((X_scaled, y.reshape(len(y), 1)))\n",
    "    column_headers = data.columns.values.tolist()\n",
    "    df_new = pd.DataFrame(data=np_new, columns=column_headers)\n",
    "    return df_new\n",
    "\n",
    "def check_num(df):\n",
    "    print(\"Class 0:\", (df.iloc[:, -1] == 0).sum())\n",
    "    print(\"Class 1:\", (df.iloc[:, -1] == 1).sum())\n",
    "\n",
    "def oversample(df):\n",
    "    if (df.iloc[:, -1] == 0).sum() >= (df.iloc[:, -1] == 1).sum():\n",
    "        df_majority = df[df.iloc[:, -1] == 0]\n",
    "        df_minority = df[df.iloc[:, -1] == 1]\n",
    "        n_major = (df.iloc[:, -1] == 0).sum()\n",
    "    else:\n",
    "        df_majority = df[df.iloc[:, -1] == 1]\n",
    "        df_minority = df[df.iloc[:, -1] == 0]\n",
    "        n_major = (df.iloc[:, -1] == 1).sum()\n",
    "\n",
    "    df_minority_oversample = resample(df_minority,\n",
    "                                      replace=True,\n",
    "                                      n_samples=n_major,\n",
    "                                      random_state=42)\n",
    "    resample_df = pd.concat([df_minority_oversample, df_majority])\n",
    "    resample_df = resample_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    return resample_df\n",
    "\n",
    "def undersample(df):\n",
    "    if (df.iloc[:, -1] == 0).sum() >= (df.iloc[:, -1] == 1).sum():\n",
    "        df_majority = df[df.iloc[:, -1] == 0]\n",
    "        df_minority = df[df.iloc[:, -1] == 1]\n",
    "        n_minor = (df.iloc[:, -1] == 1).sum()\n",
    "    else:\n",
    "        df_majority = df[df.iloc[:, -1] == 1]\n",
    "        df_minority = df[df.iloc[:, -1] == 0]\n",
    "        n_minor = (df.iloc[:, -1] == 0).sum()\n",
    "\n",
    "    df_majority_undersample = resample(df_majority,\n",
    "                                       replace=True,\n",
    "                                       n_samples=n_minor,\n",
    "                                       random_state=42)\n",
    "    resample_df = pd.concat([df_majority_undersample, df_minority])\n",
    "    resample_df = resample_df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "    return resample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoT5dWvVBZ_Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBFnTUvdL1Oz"
   },
   "source": [
    "# Apply Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6VWHsMaCIZX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBkk37vHL31J"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = load_data()\n",
    "preprocessed_data = preprocess_data(data)\n",
    "\n",
    "# Split the data\n",
    "train_x, test_x, train_y, test_y = data_split(preprocessed_data)\n",
    "\n",
    "# Scale the features\n",
    "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvg3A3B4MDxH",
    "outputId": "c1e3f73e-5117-4c21-f79e-00d5e14da950"
   },
   "outputs": [],
   "source": [
    "print(preprocessed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-l9W7Ji7MJZO",
    "outputId": "bc92b847-68ea-4396-eaea-d25311684d17"
   },
   "outputs": [],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnrO2g5AMam-",
    "outputId": "b44cab83-e302-4a6d-8801-ef346e9f5e17"
   },
   "outputs": [],
   "source": [
    "check_num(preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24vRo944Mm9M",
    "outputId": "921532c6-2d07-43bf-9f55-4237f4693d80"
   },
   "outputs": [],
   "source": [
    "scaled_df = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eSOvoeWCMtMm",
    "outputId": "b7595fcf-c8d4-45be-ce5d-1c277daf5563"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a pair plot\n",
    "sns.pairplot(preprocessed_data, hue='diabetes')\n",
    "plt.show()\n",
    "\n",
    "# Create a correlation heatmap\n",
    "correlation_matrix = preprocessed_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95yp2FoKREOd",
    "outputId": "ede45d6e-c2d9-4906-ee11-73aae1be81fa"
   },
   "outputs": [],
   "source": [
    "# balanced_data = oversample(preprocessed_data)\n",
    "# or\n",
    "balanced_data = undersample(preprocessed_data)\n",
    "\n",
    "print(balanced_data.head())\n",
    "check_num(balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGmYEqpJSgJi"
   },
   "source": [
    "#Bpnn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RDgu2q5PSP6b",
    "outputId": "adcf6f9e-ae04-410b-df2e-db5cf43bc8b9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_x, test_x, train_y, test_y = data_split(balanced_data)\n",
    "\n",
    "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
    "\n",
    "train_x = torch.from_numpy(train_x_scaled).float()\n",
    "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
    "\n",
    "test_x = torch.from_numpy(test_x_scaled).float()\n",
    "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
    "\n",
    "# Building BPNN\n",
    "class bpnn(nn.Module):\n",
    "    def __init__(self, input, hid1, hid2, hid3, numclass):\n",
    "        super(bpnn, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, hid1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features=hid1)\n",
    "        self.fc2 = nn.Linear(hid1, hid2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_features=hid2)\n",
    "        self.fc3 = nn.Linear(hid2, hid3)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(num_features=hid3)\n",
    "        self.fc4 = nn.Linear(hid3, numclass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "input = train_x.shape[1]\n",
    "hid1 = 64\n",
    "hid2 = 32\n",
    "hid3 = 16\n",
    "numclass = 1\n",
    "epochs = 2000\n",
    "batch = 128\n",
    "lrate = 0.001\n",
    "\n",
    "model = bpnn(input, hid1, hid2, hid3, numclass)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))\n",
    "    source = train_x[shuffle_indices]\n",
    "    target = train_y[shuffle_indices]\n",
    "\n",
    "    for batch_i in range(0, len(source)//batch):\n",
    "        start_i = batch_i * batch\n",
    "        source_batch = source[start_i:start_i + batch]\n",
    "        target_batch = target[start_i:start_i + batch]\n",
    "        train_pred = model(source_batch)\n",
    "        train_pred = torch.squeeze(train_pred)\n",
    "        loss = criterion(train_pred, target_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_pred_np = (train_pred.detach().numpy() > 0.5).astype(int)\n",
    "    train_acc = accuracy_score(target_batch, train_pred_np)\n",
    "\n",
    "    test_pred = model(test_x)\n",
    "    test_pred = torch.squeeze(test_pred)\n",
    "\n",
    "    test_loss = criterion(test_pred, test_y)\n",
    "\n",
    "    test_pred_np = (test_pred.detach().numpy() > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(test_y, test_pred_np)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_y, test_pred_np).ravel()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        train_loss_list.append(loss.item())\n",
    "        test_loss_list.append(test_loss.item())\n",
    "        epoch_list.append(epoch)\n",
    "\n",
    "    if test_acc >= 0.95:\n",
    "        print(\"=================================================\")\n",
    "        print(f'Training: Loss: {loss.item():.10f}, Accuracy: {train_acc}')\n",
    "        print(f'Testing: Loss: {test_loss.item():.10f}, Accuracy: {test_acc}')\n",
    "        print(f\"Testing: Sensitivity/Recall: {tp/(tp+fn):.4f}, Specificity: {tn/(tn+fp):.4f}\")\n",
    "        break\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epochs [{epoch+1}/{epochs}]')\n",
    "        print(f'Training: Loss: {loss.item():.10f}, Accuracy: {train_acc}')\n",
    "        print(f'Testing: Loss: {test_loss.item():.10f}, Accuracy: {test_acc}')\n",
    "        print(f\"Testing: Sensitivity/Recall: {tp/(tp+fn):.4f}, Specificity: {tn/(tn+fp):.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch_list, train_loss_list, label=\"training loss\", c=\"blue\")\n",
    "plt.plot(epoch_list, test_loss_list, label=\"testing loss\", c=\"orange\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best', frameon=True, facecolor='white', edgecolor='black')\n",
    "plt.title('Training and Testing Loss', size=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch_list, train_acc_list, label=\"training accuracy\", c=\"blue\")\n",
    "plt.plot(epoch_list, test_acc_list, label=\"testing accuracy\", c=\"orange\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best', frameon=True, facecolor='white', edgecolor='black')\n",
    "plt.title('Training and Testing Accuracy', size=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586,
     "referenced_widgets": [
      "dcbf1dc3dabb44ce82fb1d1394140096",
      "f7288803daeb483bbfe8fb1c7596a64b",
      "b9634463ee294feba24e8154e63d59a4",
      "21bb4ce3a5784c249558b90edf68a180",
      "336ed6751f6246c696b025ea7044c2fb",
      "fe662a11132d4345ae9fce89dae15e4d",
      "c8226e78705140898cfbfdb509de64b2",
      "2c2a76ee9f354a06a2b1678f25a45f6d",
      "26695491759c432c8d5ad9b954dd69f9",
      "9fa97520359d431f96681fa162271bc7",
      "f5198e85b653488d983dc96dee4a2382"
     ]
    },
    "id": "THERUf88FJl4",
    "outputId": "52c09ef0-0dc5-4145-9a80-036b6ff8be29"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "shap.initjs()\n",
    "\n",
    "# Create a wrapper function for your PyTorch model\n",
    "def model_wrapper(x):\n",
    "    # Convert numpy array to torch tensor\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()  # Add this line to disable Batch Normalization during inference\n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():\n",
    "        output = model(x_tensor)\n",
    "    # Return as numpy array\n",
    "    return output.numpy()\n",
    "\n",
    "# Create a background dataset for SHAP (using a subset of training data)\n",
    "background_data = train_x_scaled[:100]  # Using first 100 samples as background\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.KernelExplainer(model_wrapper, background_data)\n",
    "\n",
    "# Choose a test instance to explain\n",
    "instance_to_explain = test_x_scaled[0:1]  # Explaining first test instance\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(instance_to_explain)\n",
    "\n",
    "# Reshape shap_values to 2D if necessary\n",
    "shap_values = shap_values.reshape(shap_values.shape[0], -1) # Reshape to (number of instances, number of features)\n",
    "\n",
    "# Create summary plot\n",
    "shap.summary_plot(shap_values, instance_to_explain, feature_names=balanced_data.columns[:-1])\n",
    "\n",
    "# For a specific prediction explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], instance_to_explain[0],\n",
    "                feature_names=balanced_data.columns[:1])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
