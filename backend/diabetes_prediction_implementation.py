"""Diabetes_Prediction_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RtZqFZMYARc4Vg5WOT-10UtqzXmv3tdw
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
import os
import joblib
import json

# Set absolute paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'model.pkl')
DATASET_PATH = os.path.join(BASE_DIR, 'diabetes_prediction_dataset.csv')

def load_data():
    # Load the diabetes prediction dataset
    data = pd.read_csv(DATASET_PATH)
    return data

def preprocess_data(data):
    # Map categorical variables to numerical
    data['gender'] = data['gender'].map({'Female': 0, 'Male': 1})
    data['smoking_history'] = data['smoking_history'].map({
        'never': 0, 'No Info': 1, 'current': 2, 'former': 3, 'ever': 4, 'not current': 5
    })
    
    # Handle missing values
    # Create imputer for numerical columns
    numerical_cols = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']
    imputer = SimpleImputer(strategy='mean')
    data[numerical_cols] = imputer.fit_transform(data[numerical_cols])
    
    # For categorical columns, we already mapped them to numbers
    # so we don't need to impute them
    
    return data

def data_split(data):
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]

    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)
    return train_x, test_x, train_y, test_y

def data_scaling(train_x, test_x):
    # Handle any remaining NaN values
    train_x = train_x.fillna(train_x.mean())
    test_x = test_x.fillna(test_x.mean())
    
    # Scale the data
    ss = StandardScaler()
    ss_model = ss.fit(train_x)
    train_x_scaled = ss_model.transform(train_x)
    test_x_scaled = ss_model.transform(test_x)
    
    return train_x_scaled, test_x_scaled

def data_scaling_plot(data):
    X = data.iloc[:, :-1].to_numpy()
    y = data.iloc[:, -1].to_numpy()
    ss = StandardScaler()
    ss_model = ss.fit(X)
    X_scaled = ss_model.transform(X)
    np_new = np.hstack((X_scaled, y.reshape(len(y), 1)))
    column_headers = data.columns.values.tolist()
    df_new = pd.DataFrame(data=np_new, columns=column_headers)
    return df_new

def check_num(df):
    print("Class 0:", (df.iloc[:, -1] == 0).sum())
    print("Class 1:", (df.iloc[:, -1] == 1).sum())

def oversample(df):
    if (df.iloc[:, -1] == 0).sum() >= (df.iloc[:, -1] == 1).sum():
        df_majority = df[df.iloc[:, -1] == 0]
        df_minority = df[df.iloc[:, -1] == 1]
        n_major = (df.iloc[:, -1] == 0).sum()
    else:
        df_majority = df[df.iloc[:, -1] == 1]
        df_minority = df[df.iloc[:, -1] == 0]
        n_major = (df.iloc[:, -1] == 1).sum()

    df_minority_oversample = resample(df_minority,
                                      replace=True,
                                      n_samples=n_major,
                                      random_state=42)
    resample_df = pd.concat([df_minority_oversample, df_majority])
    resample_df = resample_df.sample(frac=1, random_state=0).reset_index(drop=True)
    return resample_df

def undersample(df):
    if (df.iloc[:, -1] == 0).sum() >= (df.iloc[:, -1] == 1).sum():
        df_majority = df[df.iloc[:, -1] == 0]
        df_minority = df[df.iloc[:, -1] == 1]
        n_minor = (df.iloc[:, -1] == 1).sum()
    else:
        df_majority = df[df.iloc[:, -1] == 1]
        df_minority = df[df.iloc[:, -1] == 0]
        n_minor = (df.iloc[:, -1] == 0).sum()

    df_majority_undersample = resample(df_majority,
                                       replace=True,
                                       n_samples=n_minor,
                                       random_state=42)
    resample_df = pd.concat([df_majority_undersample, df_minority])
    resample_df = resample_df.sample(frac=1, random_state=1).reset_index(drop=True)
    return resample_df

class DiabetesPredictor:
    def __init__(self):
        # Initialize model and scaler
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        try:
            self.model = joblib.load(MODEL_PATH)
        except Exception as e:
            raise Exception(f"Failed to load model: {str(e)}")
        
        # Initialize scaler
        try:
            train_data = load_data()
            train_data = preprocess_data(train_data)
            train_x, _, _, _ = data_split(train_data)
            self.scaler = StandardScaler()
            self.scaler.fit(train_x)
        except Exception as e:
            raise Exception(f"Failed to initialize scaler: {str(e)}")

    def predict(self, input_data):
        try:
            # Validate input data
            required_fields = ['gender', 'age', 'hypertension', 'heart_disease', 
                             'smoking_history', 'bmi', 'HbA1c_level', 'blood_glucose_level']
            if not all(field in input_data for field in required_fields):
                raise ValueError("Missing required fields in input data")

            # Convert input data to DataFrame
            input_df = pd.DataFrame([input_data])
            
            # Preprocess the input data
            input_df['gender'] = input_df['gender'].map({'Female': 0, 'Male': 1})
            input_df['smoking_history'] = input_df['smoking_history'].map({
                'never': 0, 'No Info': 1, 'current': 2, 'former': 3, 'ever': 4, 'not current': 5
            })
            
            # Scale the data
            input_scaled = self.scaler.transform(input_df)
            
            # Make prediction
            prediction = self.model.predict(input_scaled)[0]
            probabilities = self.model.predict_proba(input_scaled)[0]
            
            return {
                'prediction': int(prediction),
                'probability': float(probabilities[prediction]),
                'risk_level': 'High' if prediction == 1 else 'Low',
                'probabilities': {
                    'low_risk': float(probabilities[0]),
                    'high_risk': float(probabilities[1])
                }
            }
            
        except Exception as e:
            raise Exception(f"Prediction error: {str(e)}")
