# -*- coding: utf-8 -*-
"""Diabetes_Prediction_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RtZqFZMYARc4Vg5WOT-10UtqzXmv3tdw
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample

def load_data():
    # Load the diabetes prediction dataset
    data = pd.read_csv('diabetes_prediction_dataset.csv')
    return data

def preprocess_data(data):
    # Convert categorical variables to numerical
    data['gender'] = data['gender'].map({'Female': 0, 'Male': 1})
    data['smoking_history'] = data['smoking_history'].map({'never': 0, 'No Info': 1, 'current': 2, 'former': 3, 'ever': 4, 'not current': 5})

    return data

def data_split(data):
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]

    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)
    return train_x, test_x, train_y, test_y

def data_scaling(train_x, test_x):
    ss = StandardScaler()
    ss_model = ss.fit(train_x)
    train_x_scaled = ss_model.transform(train_x)
    test_x_scaled = ss_model.transform(test_x)
    return train_x_scaled, test_x_scaled

def data_scaling_plot(data):
    X = data.iloc[:, :-1].to_numpy()
    y = data.iloc[:, -1].to_numpy()
    ss = StandardScaler()
    ss_model = ss.fit(X)
    X_scaled = ss_model.transform(X)
    np_new = np.hstack((X_scaled, y.reshape(len(y), 1)))
    column_headers = data.columns.values.tolist()
    df_new = pd.DataFrame(data=np_new, columns=column_headers)
    return df_new

def check_num(df):
    print("Class 0:", (df.iloc[:, -1] == 0).sum())
    print("Class 1:", (df.iloc[:, -1] == 1).sum())

def oversample(df):
    if (df.iloc[:, -1] == 0).sum() >= (df.iloc[:, -1] == 1).sum():
        df_majority = df[df.iloc[:, -1] == 0]
        df_minority = df[df.iloc[:, -1] == 1]
        n_major = (df.iloc[:, -1] == 0).sum()
    else:
        df_majority = df[df.iloc[:, -1] == 1]
        df_minority = df[df.iloc[:, -1] == 0]
        n_major = (df.iloc[:, -1] == 1).sum()

    df_minority_oversample = resample(df_minority,
                                      replace=True,
                                      n_samples=n_major,
                                      random_state=42)
    resample_df = pd.concat([df_minority_oversample, df_majority])
    resample_df = resample_df.sample(frac=1, random_state=0).reset_index(drop=True)
    return resample_df

def undersample(df):
    if (df.iloc[:, -1] == 0).sum() >= (df.iloc[:, -1] == 1).sum():
        df_majority = df[df.iloc[:, -1] == 0]
        df_minority = df[df.iloc[:, -1] == 1]
        n_minor = (df.iloc[:, -1] == 1).sum()
    else:
        df_majority = df[df.iloc[:, -1] == 1]
        df_minority = df[df.iloc[:, -1] == 0]
        n_minor = (df.iloc[:, -1] == 0).sum()

    df_majority_undersample = resample(df_majority,
                                       replace=True,
                                       n_samples=n_minor,
                                       random_state=42)
    resample_df = pd.concat([df_majority_undersample, df_minority])
    resample_df = resample_df.sample(frac=1, random_state=1).reset_index(drop=True)
    return resample_df



"""# Apply Functions"""

# Load and preprocess the data
data = load_data()
preprocessed_data = preprocess_data(data)

# Split the data
train_x, test_x, train_y, test_y = data_split(preprocessed_data)

# Scale the features
train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)

print(preprocessed_data.head())

preprocessed_data

check_num(preprocessed_data)

scaled_df = pd.DataFrame(train_x_scaled, columns=train_x.columns)
print(scaled_df.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Create a pair plot
sns.pairplot(preprocessed_data, hue='diabetes')
plt.show()

# Create a correlation heatmap
correlation_matrix = preprocessed_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# balanced_data = oversample(preprocessed_data)
# or
balanced_data = undersample(preprocessed_data)

print(balanced_data.head())
check_num(balanced_data)

"""#Bpnn Model"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

# Prepare training and testing data
train_x, test_x, train_y, test_y = data_split(balanced_data)

train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)

train_x = torch.from_numpy(train_x_scaled).float()
train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())

test_x = torch.from_numpy(test_x_scaled).float()
test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())

# Building BPNN
class bpnn(nn.Module):
    def __init__(self, input, hid1, hid2, hid3, numclass):
        super(bpnn, self).__init__()
        self.fc1 = nn.Linear(input, hid1)
        self.batch_norm1 = nn.BatchNorm1d(num_features=hid1)
        self.fc2 = nn.Linear(hid1, hid2)
        self.batch_norm2 = nn.BatchNorm1d(num_features=hid2)
        self.fc3 = nn.Linear(hid2, hid3)
        self.batch_norm3 = nn.BatchNorm1d(num_features=hid3)
        self.fc4 = nn.Linear(hid3, numclass)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.batch_norm1(x)
        x = F.relu(self.fc2(x))
        x = self.batch_norm2(x)
        x = F.relu(self.fc3(x))
        x = self.batch_norm3(x)
        x = torch.sigmoid(self.fc4(x))
        return x

input = train_x.shape[1]
hid1 = 64
hid2 = 32
hid3 = 16
numclass = 1
epochs = 2000
batch = 128
lrate = 0.001

model = bpnn(input, hid1, hid2, hid3, numclass)

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lrate)

train_loss_list = []
test_loss_list = []
train_acc_list = []
test_acc_list = []
epoch_list = []

for epoch in range(epochs):
    shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))
    source = train_x[shuffle_indices]
    target = train_y[shuffle_indices]

    for batch_i in range(0, len(source)//batch):
        start_i = batch_i * batch
        source_batch = source[start_i:start_i + batch]
        target_batch = target[start_i:start_i + batch]
        train_pred = model(source_batch)
        train_pred = torch.squeeze(train_pred)
        loss = criterion(train_pred, target_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    train_pred_np = (train_pred.detach().numpy() > 0.5).astype(int)
    train_acc = accuracy_score(target_batch, train_pred_np)

    test_pred = model(test_x)
    test_pred = torch.squeeze(test_pred)

    test_loss = criterion(test_pred, test_y)

    test_pred_np = (test_pred.detach().numpy() > 0.5).astype(int)
    test_acc = accuracy_score(test_y, test_pred_np)

    tn, fp, fn, tp = confusion_matrix(test_y, test_pred_np).ravel()

    if (epoch+1) % 10 == 0:
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        train_loss_list.append(loss.item())
        test_loss_list.append(test_loss.item())
        epoch_list.append(epoch)

    if test_acc >= 0.95:
        print("=================================================")
        print(f'Training: Loss: {loss.item():.10f}, Accuracy: {train_acc}')
        print(f'Testing: Loss: {test_loss.item():.10f}, Accuracy: {test_acc}')
        print(f"Testing: Sensitivity/Recall: {tp/(tp+fn):.4f}, Specificity: {tn/(tn+fp):.4f}")
        break

    if (epoch+1) % 100 == 0:
        print(f'Epochs [{epoch+1}/{epochs}]')
        print(f'Training: Loss: {loss.item():.10f}, Accuracy: {train_acc}')
        print(f'Testing: Loss: {test_loss.item():.10f}, Accuracy: {test_acc}')
        print(f"Testing: Sensitivity/Recall: {tp/(tp+fn):.4f}, Specificity: {tn/(tn+fp):.4f}")

# Plot results
plt.figure(figsize=(16, 5))

plt.subplot(121)
plt.plot(epoch_list, train_loss_list, label="training loss", c="blue")
plt.plot(epoch_list, test_loss_list, label="testing loss", c="orange")
plt.xlabel('epoch')
plt.ylabel('Loss')
plt.legend(loc='best', frameon=True, facecolor='white', edgecolor='black')
plt.title('Training and Testing Loss', size=12)

plt.subplot(122)
plt.plot(epoch_list, train_acc_list, label="training accuracy", c="blue")
plt.plot(epoch_list, test_acc_list, label="testing accuracy", c="orange")
plt.xlabel('epoch')
plt.ylabel('Accuracy')
plt.legend(loc='best', frameon=True, facecolor='white', edgecolor='black')
plt.title('Training and Testing Accuracy', size=12)

plt.show()

import shap
import numpy as np
import torch
shap.initjs()

# Create a wrapper function for your PyTorch model
def model_wrapper(x):
    # Convert numpy array to torch tensor
    x_tensor = torch.tensor(x, dtype=torch.float32)
    # Set the model to evaluation mode
    model.eval()  # Add this line to disable Batch Normalization during inference
    # Get predictions from the model
    with torch.no_grad():
        output = model(x_tensor)
    # Return as numpy array
    return output.numpy()

# Create a background dataset for SHAP (using a subset of training data)
background_data = train_x_scaled[:100]  # Using first 100 samples as background

# Initialize the SHAP explainer
explainer = shap.KernelExplainer(model_wrapper, background_data)

# Choose a test instance to explain
instance_to_explain = test_x_scaled[0:1]  # Explaining first test instance

# Calculate SHAP values
shap_values = explainer.shap_values(instance_to_explain)

# Reshape shap_values to 2D if necessary
shap_values = shap_values.reshape(shap_values.shape[0], -1) # Reshape to (number of instances, number of features)

# Create summary plot
shap.summary_plot(shap_values, instance_to_explain, feature_names=balanced_data.columns[:-1])

# For a specific prediction explanation
shap.force_plot(explainer.expected_value, shap_values[0], instance_to_explain[0],
                feature_names=balanced_data.columns[:1])